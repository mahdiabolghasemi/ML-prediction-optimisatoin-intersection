
@article{mylne2002decision,
  title={Decision-making from probability forecasts based on forecast value},
  author={Mylne, Kenneth R},
  journal={Meteorological Applications},
  volume={9},
  number={3},
  pages={307--315},
  year={2002},
  publisher={Cambridge University Press}
}

@article{guo2013multivariate,
  title={A multivariate intelligent decision-making model for retail sales forecasting},
  author={Guo, ZX and Wong, Wai Keung and Li, Min},
  journal={Decision Support Systems},
  volume={55},
  number={1},
  pages={247--255},
  year={2013},
  publisher={Elsevier}
}


@article{goodwin2009common,
  title={Common sense and hard decision analysis: why might they conflict?},
  author={Goodwin, Paul},
  journal={Management Decision},
  year={2009},
  publisher={Emerald Group Publishing Limited}
}



@article{gitmahdi,
  title={How to predict and optimise with asymmetric error metrics},
  author={ Abolghasemi, Mahdi and Bean, Richard},
  journal= {GitHub, {PREPRINT}},
  year= {2022},
url={https://github.com/mahdiabolghasemi/IEEE_forecast_optimise_association/blob/main/manuscript.pdf}
}

@aricle{giturban,
  title={Forecast and optimise with machine learning: applications in wastewater treatment},
  author={Colwell, Matthew and Abolghasemi, Mahdi },
  journal= {Australian Data Science Network Conference, {[PREPRINT OR COMPARABLE]}},
  year= {2022},
url={https://github.com/mahdiabolghasemi/wastewater_treatment/blob/main/Resource\%20Recovery\%20Digital\%20Enablement\%20Extended\%20Abstract.pdf}
}



@article{abolghasemi2021state,
  title={State-of-the-art predictive and prescriptive analytics for IEEE CIS 3rd Technical Challenge},
  author={Abolghasemi, Mahdi and Esmaeilbeigi, Rasul},
  journal={arXiv preprint arXiv:2112.03595},
  year={2021}
}


@article{abolghasemi2021effectively,
  title={How to effectively use machine learning models to predict the solutions for optimization problems: lessons from loss function},
  author={Abolghasemi, Mahdi and Abbasi, Babak and Babaei, Toktam and HosseiniFard, Zahra},
  journal={arXiv preprint arXiv:2105.06618},
  year={2021}
}

@article{donti2021dc3,
  title={Dc3: A learning method for optimization with hard constraints},
  author={Donti, Priya L and Rolnick, David and Kolter, J Zico},
  journal={arXiv preprint arXiv:2104.12225},
  year={2021}
}

@article{larsen2022predicting,
  title={Predicting tactical solutions to operational planning problems under imperfect information},
  author={Larsen, Eric and Lachapelle, S{\'e}bastien and Bengio, Yoshua and Frejinger, Emma and Lacoste-Julien, Simon and Lodi, Andrea},
  journal={INFORMS Journal on Computing},
  volume={34},
  number={1},
  pages={227--242},
  year={2022},
  publisher={INFORMS}
}

@article{chatzos2020high,
  title={High-fidelity machine learning approximations of large-scale optimal power flow},
  author={Chatzos, Minas and Fioretto, Ferdinando and Mak, Terrence WK and Van Hentenryck, Pascal},
  journal={arXiv preprint arXiv:2006.16356},
  year={2020}
}



@inproceedings{elmachtoub2020decision,
  title={Decision trees for decision-making under the predict-then-optimize framework},
  author={Elmachtoub, Adam and Liang, Jason Cheuk Nam and McNellis, Ryan},
  booktitle={International Conference on Machine Learning},
  pages={2858--2867},
  year={2020},
  organization={PMLR}
}

@inproceedings{mandi2022decision,
  title={Decision-Focused Learning: Through the Lens of Learning to Rank},
  author={Mandi, Jayanta and Bucarey, V{\i}́ctor and Tchomba, Maxime Mulamba Ke and Guns, Tias},
  booktitle={International Conference on Machine Learning},
  pages={14935--14947},
  year={2022},
  organization={PMLR}
}
@inproceedings{detassis2021teaching,
  title={Teaching the old dog new tricks: Supervised learning with constraints},
  author={Detassis, Fabrizio and Lombardi, Michele and Milano, Michela},
  booktitle={Proceedings of the AAAI Conference on Artificial Intelligence},
  volume={35},
  number={5},
  pages={3742--3749},
  year={2021}
}

@article{donti2017task,
  title={Task-based end-to-end model learning in stochastic optimization},
  author={Donti, Priya and Amos, Brandon and Kolter, J Zico},
  journal={Advances in neural information processing systems},
  volume={30},
  year={2017}
}

@article{mulamba2020contrastive,
  title={Contrastive losses and solution caching for predict-and-optimize},
  author={Mulamba, Maxime and Mandi, Jayanta and Diligenti, Michelangelo and Lombardi, Michele and Bucarey, Victor and Guns, Tias},
  journal={Proceedings of the Thirtieth International Joint Conference on Artificial Intelligence (IJCAI-21)},
  year={2021}
}

@article{shah2022learning,
  title={Learning (Local) Surrogate Loss Functions for Predict-Then-Optimize Problems},
  author={Shah, Sanket and Wilder, Bryan and Perrault, Andrew and Tambe, Milind},
  journal={arXiv preprint arXiv:2203.16067},
  year={2022}
}
@inproceedings{wilder2019melding,
  title={Melding the data-decisions pipeline: Decision-focused learning for combinatorial optimization},
  author={Wilder, Bryan and Dilkina, Bistra and Tambe, Milind},
  booktitle={Proceedings of the AAAI Conference on Artificial Intelligence},
  volume={33},
  number={01},
  pages={1658--1665},
  year={2019}
}

@article{elmachtoub2022smart,
  title={Smart “predict, then optimize”},
  author={Elmachtoub, Adam N and Grigas, Paul},
  journal={Management Science},
  volume={68},
  number={1},
  pages={9--26},
  year={2022},
  publisher={INFORMS}
}

@inproceedings{mandi2020smart,
  title={Smart predict-and-optimize for hard combinatorial optimization problems},
  author={Mandi, Jayanta and Stuckey, Peter J and Guns, Tias and others},
  booktitle={Proceedings of the AAAI Conference on Artificial Intelligence},
  volume={34},
  number={02},
  pages={1603--1610},
  year={2020}
}

@inproceedings{demirovic2019investigation,
  title={An investigation into prediction+ optimisation for the knapsack problem},
  author={Demirovi{\'c}, Emir and Stuckey, Peter J and Bailey, James and Chan, Jeffrey and Leckie, Chris and Ramamohanarao, Kotagiri and Guns, Tias},
  booktitle={International Conference on Integration of Constraint Programming, Artificial Intelligence, and Operations Research},
  pages={241--257},
  year={2019},
  organization={Springer}
}

@article{cappart2021combinatorial,
  title={Combinatorial optimization and reasoning with graph neural networks},
  author={Cappart, Quentin and Ch{\'e}telat, Didier and Khalil, Elias and Lodi, Andrea and Morris, Christopher and Veli{\v{c}}kovi{\'c}, Petar},
  journal={arXiv preprint arXiv:2102.09544},
  year={2021}
}

@article{vinyals2015pointer,
  title={Pointer networks},
  author={Vinyals, Oriol and Fortunato, Meire and Jaitly, Navdeep},
  journal={Advances in neural information processing systems},
  volume={28},
  year={2015}
}

@inproceedings{detassis2021teaching,
  title={Teaching the old dog new tricks: Supervised learning with constraints},
  author={Detassis, Fabrizio and Lombardi, Michele and Milano, Michela},
  booktitle={Proceedings of the AAAI Conference on Artificial Intelligence},
  volume={35},
  number={5},
  pages={3742--3749},
  year={2021}
}

@inproceedings{fioretto2020lagrangian,
  title={Lagrangian duality for constrained deep learning},
  author={Fioretto, Ferdinando and Hentenryck, Pascal Van and Mak, Terrence WK and Tran, Cuong and Baldo, Federico and Lombardi, Michele},
  booktitle={Joint European Conference on Machine Learning and Knowledge Discovery in Databases},
  pages={118--135},
  year={2020},
  organization={Springer}
}

@inproceedings{tran2021differentially,
  title={Differentially private and fair deep learning: A lagrangian dual approach},
  author={Tran, Cuong and Fioretto, Ferdinando and Van Hentenryck, Pascal},
  booktitle={Proceedings of the AAAI Conference on Artificial Intelligence},
  volume={35},
  number={11},
  pages={9932--9939},
  year={2021}
}


@inproceedings{de2018learning,
  title={Learning constraints from examples},
  author={De Raedt, Luc and Passerini, Andrea and Teso, Stefano},
  booktitle={Proceedings of the AAAI Conference on Artificial Intelligence},
  volume={32},
  number={1},
  year={2018}
}

@inproceedings{brys2015policy,
  title={Policy Transfer using Reward Shaping.},
  author={Brys, Tim and Harutyunyan, Anna and Taylor, Matthew E and Now{\'e}, Ann},
  booktitle={AAMAS},
  pages={181--188},
  year={2015}
}
@article{marcos2015machine,
  title={Machine learning to balance the load in parallel branch-and-bound},
  author={Marcos Alvarez, Alejandro and Wehenkel, Louis and Louveaux, Quentin},
  year={2015}
}



@article{bertsimas2016best,
  title={Best subset selection via a modern optimization lens},
  author={Bertsimas, Dimitris and King, Angela and Mazumder, Rahul},
  journal={The annals of statistics},
  volume={44},
  number={2},
  pages={813--852},
  year={2016},
  publisher={Institute of Mathematical Statistics}
}


@inproceedings{Fioretto:ECML20,
    title     = {A Lagrangian Dual Framework for Deep Neural Networks with Constraints Optimization},
    author    = {Ferdinando Fioretto and  Pascal {Van  Hentenryck} and Terrence {W.K. Mak} and Cuong Tran and Federico Baldo and Michele Lombardi},
    booktitle = {European Conference on Machine Learning and  Principles and Practice of Knowledge Discovery in Databases ({ECML-PKDD})},
    year      = {2020},
    series    = {Lecture Notes in Computer Science},
    volume    = {12461},
    pages     = {118--135},
    publisher = {Springer},
 
}

@article{bello2016neural,
  title={Neural combinatorial optimization with reinforcement learning},
  author={Bello, Irwan and Pham, Hieu and Le, Quoc V and Norouzi, Mohammad and Bengio, Samy},
  journal={arXiv preprint arXiv:1611.09940},
  year={2016}
}

@article{duvenaud2015advances,
  title={Advances in Neural Information Processing Systems 28},
  author={Duvenaud, David K and Maclaurin, Dougal and Iparraguirre, Jorge and Bombarell, Rafael and Hirzel, Timothy and Aspuru-Guzik, Alan and Adams, Ryan P},
  journal={Cortes C., Lawrence ND, Lee DD, Sugiyama M., Garnett R., Eds},
  pages={2224--2232},
  year={2015}
}

@book{bertsimas2019machine,
  title={Machine learning under a modern optimization lens},
  author={Bertsimas, Dimitris and Dunn, Jack},
  year={2019},
  publisher={Dynamic Ideas LLC}
}

@article{bertsimas2017optimal,
  title={Optimal classification trees},
  author={Bertsimas, Dimitris and Dunn, Jack},
  journal={Machine Learning},
  volume={106},
  number={7},
  pages={1039--1082},
  year={2017},
  publisher={Springer}
}

@INPROCEEDINGS{Zhao2020-lv,
  title     = "{DeepOPF+}: A Deep Neural Network Approach for {DC} Optimal
               Power Flow for Ensuring Feasibility",
  booktitle = "2020 {IEEE} International Conference on Communications, Control,
               and Computing Technologies for Smart Grids ({SmartGridComm})",
  author    = "Zhao, Tianyu and Pan, Xiang and Chen, Minghua and Venzke,
               Andreas and Low, Steven H",
  pages     = "1--6",
  month     =  nov,
  year      =  2020,
  keywords  = "Training;Calibration;Smart grids;Generators;Optimization;Power
               transmission lines;Approximation error"
}

@ARTICLE{Pan2020-cm,
  title         = "{DeepOPF}: A {Feasibility-Optimized} Deep Neural Network
                   Approach for {AC} Optimal Power Flow Problems",
  author        = "Pan, Xiang and Chen, Minghua and Zhao, Tianyu and Low,
                   Steven H",
  month         =  jul,
  year          =  2020,
  archivePrefix = "arXiv",
  primaryClass  = "eess.SY",
  eprint        = "2007.01002"
}

@ARTICLE{Pan2021-rv,
  title    = "{DeepOPF}: A Deep Neural Network Approach for
              {Security-Constrained} {DC} Optimal Power Flow",
  author   = "Pan, Xiang and Zhao, Tianyu and Chen, Minghua and Zhang, Shengyu",
 
  journal  = "IEEE Trans. Power Syst.",
  volume   =  36,
  number   =  3,
  pages    = "1725--1735",
  month    =  may,
  year     =  2021,
  keywords = "Power system stability;Complexity theory;Generators;Machine
              learning;Biological neural networks;Deep learning;deep neural
              network;optimal power flow"
}

@ARTICLE{Gould2019-tl,
  title         = "Deep Declarative Networks: A New Hope",
  author        = "Gould, Stephen and Hartley, Richard and Campbell, Dylan",
 
  month         =  sep,
  year          =  2019,
  archivePrefix = "arXiv",
  primaryClass  = "cs.LG",
  eprint        = "1909.04866"
}

@ARTICLE{Wilder_undated-uc,
  title   = "End to end learning and optimization on graphs",
  author  = "{Wilder} and {Ewing} and {Dilkina} and {others}",
  journal = "Adv. Neural Inf. Process. Syst."
}

@ARTICLE{Modaresi2020-os,
  title     = "Learning in Combinatorial Optimization: What and how to Explore",
  author    = "Modaresi, Sajad and Saur{\'e}, Denis and Vielma, Juan Pablo",
 
  journal   = "Oper. Res.",
  publisher = "Institute for Operations Research and the Management Sciences
               (INFORMS)",
  volume    =  68,
  number    =  5,
  pages     = "1585--1604",
  month     =  sep,
  year      =  2020,
  language  = "en"
}

@ARTICLE{Li2021-ez,
  title    = "Deep Reinforcement Learning for Combinatorial Optimization:
              Covering Salesman Problems",
  author   = "Li, Kaiwen and Zhang, Tao and Wang, Rui and Wang, Yuheng and Han,
              Yi and Wang, Ling",
   
  journal  = "IEEE Trans Cybern",
  volume   = "PP",
  month    =  aug,
  year     =  2021,
  language = "en"
}

@ARTICLE{Wilder2019-wq,
  title    = "Melding the {Data-Decisions} Pipeline: {Decision-Focused}
              Learning for Combinatorial Optimization",
  author   = "Wilder, Bryan and Dilkina, Bistra and Tambe, Milind",
   
  journal  = "AAAI",
  volume   =  33,
  number   =  01,
  pages    = "1658--1665",
  month    =  jul,
  year     =  2019,
  language = "en"
}

@ARTICLE{Mazyavkina2021-tl,
  title    = "Reinforcement learning for combinatorial optimization: A survey",
  author   = "Mazyavkina, Nina and Sviridov, Sergey and Ivanov, Sergei and
              Burnaev, Evgeny",
  abstract = "Many traditional algorithms for solving combinatorial
              optimization problems involve using hand-crafted heuristics that
              sequentially construct a solution. Such heuristics are designed
              by domain experts and may often be suboptimal due to the hard
              nature of the problems. Reinforcement learning (RL) proposes a
              good alternative to automate the search of these heuristics by
              training an agent in a supervised or self-supervised manner. In
              this survey, we explore the recent advancements of applying RL
              frameworks to hard combinatorial problems. Our survey provides
              the necessary background for operations research and machine
              learning communities and showcases the works that are moving the
              field forward. We juxtapose recently proposed RL methods, laying
              out the timeline of the improvements for each problem, as well as
              we make a comparison with traditional algorithms, indicating that
              RL models can become a promising direction for solving
              combinatorial problems.",
  journal  = "Comput. Oper. Res.",
  volume   =  134,
  pages    = "105400",
  month    =  oct,
  year     =  2021,
  keywords = "Reinforcement learning; Operations research; Combinatorial
              optimization; Value-based methods; Policy-based methods"
}

@MISC{Kotary2022-dx,
  title   = "{End-to-End} Learning for Fair Ranking Systems",
  author  = "Kotary, James and Fioretto, Ferdinando and Van Hentenryck, Pascal
             and Zhu, Ziwei",
  journal = "Proceedings of the ACM Web Conference 2022",
  year    =  2022
}

@MISC{noauthor_undated-ss,
  title = "water waste thesis.pdf"
}

@ARTICLE{Pinheiro_undated-sv,
  title  = "Unconstrained Parameterizations for {Variance-Covariance} Matrices",
  author = "Pinheiro, Jose C and Bates, Douglas M"
}

@ARTICLE{Lim_undated-zy,
  title  = "{USING} {NEURAL} {NETWORKS} {IN} {CONSTRAINED} {OPTIMIZATION}
            {PROBLEMS}",
  author = "Lim, Jan"
}

@ARTICLE{Spieker_undated-xr,
  title  = "Predictive Machine Learning of Objective Boundaries for Solving
            {COPs}",
  author = "Spieker, Helge and Gotlieb, Arnaud"
}

@ARTICLE{Larsen2022-hm,
  title     = "Predicting Tactical Solutions to Operational Planning Problems
               Under Imperfect Information",
  author    = "Larsen, Eric and Lachapelle, S{\'e}bastien and Bengio, Yoshua
               and Frejinger, Emma and Lacoste-Julien, Simon and Lodi, Andrea",
  abstract  = "This paper offers a methodological contribution at the
               intersection of machine learning and operations research.
               Namely, we propose a methodology to quickly predict expected
               tactical descriptions of operational solutions (TDOSs). The
               problem we address occurs in the context of two-stage stochastic
               programming, where the second stage is demanding
               computationally. We aim to predict at a high speed the expected
               TDOS associated with the second-stage problem, conditionally on
               the first-stage variables. This may be used in support of the
               solution to the overall two-stage problem by avoiding the online
               generation of multiple second-stage scenarios and solutions. We
               formulate the tactical prediction problem as a stochastic
               optimal prediction program, whose solution we approximate with
               supervised machine learning. The training data set consists of a
               large number of deterministic operational problems generated by
               controlled probabilistic sampling. The labels are computed based
               on solutions to these problems (solved independently and
               offline), employing appropriate aggregation and subselection
               methods to address uncertainty. Results on our motivating
               application on load planning for rail transportation show that
               deep learning models produce accurate predictions in very short
               computing time (milliseconds or less). The predictive accuracy
               is close to the lower bounds calculated based on sample average
               approximation of the stochastic prediction programs.",
  journal   = "INFORMS J. Comput.",
  publisher = "INFORMS",
  volume    =  34,
  number    =  1,
  pages     = "227--242",
  month     =  jan,
  year      =  2022
}

@ARTICLE{Fioretto2020-fx,
  title     = "Predicting {AC} optimal power flows: Combining deep learning and
               Lagrangian dual methods",
  author    = "Fioretto, Ferdinando and Mak, Terrence W K and Van Hentenryck,
               Pascal",
  abstract  = "The Optimal Power Flow (OPF) problem is a fundamental building
               block for the optimization of electrical power systems. It is
               nonlinear and nonconvex and computes the generator setpoints for
               power and voltage, given a set of load demands. It is often
               solved repeatedly under various conditions, either in real-time
               or in large-scale studies. This need is further exacerbated by
               the increasing stochasticity of power systems due to renewable
               energy sources in front and behind the meter. To address these
               challenges, this paper presents a deep learning approach to the
               OPF. The learning model exploits the information available in
               the similar states of the system (which is commonly available in
               practical applications), as well as a dual Lagrangian method to
               satisfy the physical and engineering constraints present in the
               OPF. The proposed model is evaluated on a large collection of
               realistic medium-sized power systems. The experimental results
               show that its predictions are highly accurate with average
               errors as low as 0.2\%. Additionally, the proposed approach is
               shown to improve the accuracy of the widely adopted linear DC
               approximation by at least two orders of magnitude.",
  journal   = "Proc. Conf. AAAI Artif. Intell.",
  publisher = "Association for the Advancement of Artificial Intelligence
               (AAAI)",
  volume    =  34,
  number    =  01,
  pages     = "630--637",
  month     =  apr,
  year      =  2020
}

@ARTICLE{Bello2016-jg,
  title         = "Neural Combinatorial Optimization with Reinforcement
                   Learning",
  author        = "Bello, Irwan and Pham, Hieu and Le, Quoc V and Norouzi,
                   Mohammad and Bengio, Samy",
  abstract      = "This paper presents a framework to tackle combinatorial
                   optimization problems using neural networks and
                   reinforcement learning. We focus on the traveling salesman
                   problem (TSP) and train a recurrent network that, given a
                   set of city coordinates, predicts a distribution over
                   different city permutations. Using negative tour length as
                   the reward signal, we optimize the parameters of the
                   recurrent network using a policy gradient method. We compare
                   learning the network parameters on a set of training graphs
                   against learning them on individual test graphs. Despite the
                   computational expense, without much engineering and
                   heuristic designing, Neural Combinatorial Optimization
                   achieves close to optimal results on 2D Euclidean graphs
                   with up to 100 nodes. Applied to the KnapSack, another
                   NP-hard problem, the same method obtains optimal solutions
                   for instances with up to 200 items.",
  month         =  nov,
  year          =  2016,
  archivePrefix = "arXiv",
  primaryClass  = "cs.AI",
  eprint        = "1611.09940"
}

@ARTICLE{Silver2016-wn,
  title    = "Mastering the game of Go with deep neural networks and tree
              search",
  author   = "Silver, David and Huang, Aja and Maddison, Chris J and Guez,
              Arthur and Sifre, Laurent and van den Driessche, George and
              Schrittwieser, Julian and Antonoglou, Ioannis and Panneershelvam,
              Veda and Lanctot, Marc and Dieleman, Sander and Grewe, Dominik
              and Nham, John and Kalchbrenner, Nal and Sutskever, Ilya and
              Lillicrap, Timothy and Leach, Madeleine and Kavukcuoglu, Koray
              and Graepel, Thore and Hassabis, Demis",
  abstract = "The game of Go has long been viewed as the most challenging of
              classic games for artificial intelligence owing to its enormous
              search space and the difficulty of evaluating board positions and
              moves. Here we introduce a new approach to computer Go that uses
              'value networks' to evaluate board positions and 'policy
              networks' to select moves. These deep neural networks are trained
              by a novel combination of supervised learning from human expert
              games, and reinforcement learning from games of self-play.
              Without any lookahead search, the neural networks play Go at the
              level of state-of-the-art Monte Carlo tree search programs that
              simulate thousands of random games of self-play. We also
              introduce a new search algorithm that combines Monte Carlo
              simulation with value and policy networks. Using this search
              algorithm, our program AlphaGo achieved a 99.8\% winning rate
              against other Go programs, and defeated the human European Go
              champion by 5 games to 0. This is the first time that a computer
              program has defeated a human professional player in the
              full-sized game of Go, a feat previously thought to be at least a
              decade away.",
  journal  = "Nature",
  volume   =  529,
  number   =  7587,
  pages    = "484--489",
  month    =  jan,
  year     =  2016,
  language = "en"
}

% The entry below contains non-ASCII chars that could not be converted
% to a LaTeX equivalent.
@ARTICLE{Daume_undated-vo,
  title  = "Learning to Search in {Branch-and-Bound} Algorithms⇤",
  author = "Daume, III, He He Hal and Eisner, Jason"
}

@ARTICLE{Sutton_undated-rs,
  title   = "Learning to Predict by the Methods of Temporal Differences",
  author  = "{Sutton}",
  journal = "Mach. Learn."
}

@ARTICLE{Bayati2021-hr,
  title         = "Learning to Optimize Under Constraints with Unsupervised
                   Deep Neural Networks",
  author        = "Bayati, Seyedrazieh and Jabbarvaziri, Faramarz",
  abstract      = "In this paper, we propose a machine learning (ML) method to
                   learn how to solve a generic constrained continuous
                   optimization problem. To the best of our knowledge, the
                   generic methods that learn to optimize, focus on
                   unconstrained optimization problems and those dealing with
                   constrained problems are not easy-to-generalize. This
                   approach is quite useful in optimization tasks where the
                   problem's parameters constantly change and require resolving
                   the optimization task per parameter update. In such
                   problems, the computational complexity of optimization
                   algorithms such as gradient descent or interior point method
                   preclude near-optimal designs in real-time applications. In
                   this paper, we propose an unsupervised deep learning (DL)
                   solution for solving constrained optimization problems in
                   real-time by relegating the main computation load to offline
                   training phase. This paper's main contribution is proposing
                   a method for enforcing the equality and inequality
                   constraints to the DL-generated solutions for generic
                   optimization tasks.",
  month         =  jan,
  year          =  2021,
  archivePrefix = "arXiv",
  primaryClass  = "cs.LG",
  eprint        = "2101.00744"
}

@ARTICLE{Khalil_undated-qm,
  title  = "Learning to Branch in Mixed Integer Programming",
  author = "Khalil, Elias B and Le Bodic, Pierre and Song, Le and Nemhauser,
            George and Dilkina, Bistra"
}

@ARTICLE{Dai_undated-wp,
  title  = "Learning Combinatorial Optimization Algorithms over Graphs",
  author = "Dai, Hanjun and Khalil, Elias B and Zhang, Yuyu and Dilkina, Bistra
            and Song, Le"
}

@ARTICLE{Hussein2017-tb,
  title     = "Imitation Learning: A Survey of Learning Methods",
  author    = "Hussein, Ahmed and Gaber, Mohamed Medhat and Elyan, Eyad and
               Jayne, Chrisina",
  abstract  = "Imitation learning techniques aim to mimic human behavior in a
               given task. An agent (a learning machine) is trained to perform
               a task from demonstrations by learning a mapping between
               observations and actions. The idea of teaching by imitation has
               been around for many years; however, the field is gaining
               attention recently due to advances in computing and sensing as
               well as rising demand for intelligent applications. The paradigm
               of learning by imitation is gaining popularity because it
               facilitates teaching complex tasks with minimal expert knowledge
               of the tasks. Generic imitation learning methods could
               potentially reduce the problem of teaching a task to that of
               providing demonstrations, without the need for explicit
               programming or designing reward functions specific to the task.
               Modern sensors are able to collect and transmit high volumes of
               data rapidly, and processors with high computational power allow
               fast processing that maps the sensory data to actions in a
               timely manner. This opens the door for many potential AI
               applications that require real-time perception and reaction such
               as humanoid robots, self-driving vehicles, human computer
               interaction, and computer games, to name a few. However,
               specialized algorithms are needed to effectively and robustly
               learn models as learning by imitation poses its own set of
               challenges. In this article, we survey imitation learning
               methods and present design options in different steps of the
               learning process. We introduce a background and motivation for
               the field as well as highlight challenges specific to the
               imitation problem. Methods for designing and evaluating
               imitation learning tasks are categorized and reviewed. Special
               attention is given to learning methods in robotics and games as
               these domains are the most popular in the literature and provide
               a wide array of problems and methodologies. We extensively
               discuss combining imitation learning approaches using different
               sources and methods, as well as incorporating other motion
               learning methods to enhance imitation. We also discuss the
               potential impact on industry, present major applications, and
               highlight current and future research directions.",
  journal   = "ACM Comput. Surv.",
  publisher = "Association for Computing Machinery",
  volume    =  50,
  number    =  2,
  pages     = "1--35",
  month     =  apr,
  year      =  2017,
  address   = "New York, NY, USA",
  keywords  = "learning from demonstrations, intelligent agents, feature
               representations, Imitation learning, deep learning,
               self-improvement, robotics, reinforcement learning, learning
               from experience"
}

@ARTICLE{Chatzos2020-dh,
  title         = "{High-Fidelity} Machine Learning Approximations of
                   {Large-Scale} Optimal Power Flow",
  author        = "Chatzos, Minas and Fioretto, Ferdinando and Mak, Terrence W
                   K and Van Hentenryck, Pascal",
  abstract      = "The AC Optimal Power Flow (AC-OPF) is a key building block
                   in many power system applications. It determines generator
                   setpoints at minimal cost that meet the power demands while
                   satisfying the underlying physical and operational
                   constraints. It is non-convex and NP-hard, and
                   computationally challenging for large-scale power systems.
                   Motivated by the increased stochasticity in generation
                   schedules and increasing penetration of renewable sources,
                   this paper explores a deep learning approach to deliver
                   highly efficient and accurate approximations to the AC-OPF.
                   In particular, the paper proposes an integration of deep
                   neural networks and Lagrangian duality to capture the
                   physical and operational constraints. The resulting model,
                   called OPF-DNN, is evaluated on real case studies from the
                   French transmission system, with up to 3,400 buses and 4,500
                   lines. Computational results show that OPF-DNN produces
                   highly accurate AC-OPF approximations whose costs are within
                   0.01\% of optimality. OPF-DNN generates, in milliseconds,
                   solutions that capture the problem constraints with high
                   fidelity.",
  month         =  jun,
  year          =  2020,
  archivePrefix = "arXiv",
  primaryClass  = "eess.SP",
  eprint        = "2006.16356"
}

@ARTICLE{Bertsimas2020-li,
  title     = "From Predictive to Prescriptive Analytics",
  author    = "Bertsimas, Dimitris and Kallus, Nathan",
  abstract  = "We combine ideas from machine learning (ML) and operations
               research and management science (OR/MS) in developing a
               framework, along with specific methods, for using data to
               prescribe optimal decisions in OR/MS problems. In a departure
               from other work on data-driven optimization, we consider data
               consisting, not only of observations of quantities with direct
               effect on costs/revenues, such as demand or returns, but also
               predominantly of observations of associated auxiliary
               quantities. The main problem of interest is a conditional
               stochastic optimization problem, given imperfect observations,
               where the joint probability distributions that specify the
               problem are unknown. We demonstrate how our proposed methods are
               generally applicable to a wide range of decision problems and
               prove that they are computationally tractable and asymptotically
               optimal under mild conditions, even when data are not
               independent and identically distributed and for censored
               observations. We extend these to the case in which some decision
               variables, such as price, may affect uncertainty and their
               causal effects are unknown. We develop the coefficient of
               prescriptiveness P to measure the prescriptive content of data
               and the efficacy of a policy from an operations perspective. We
               demonstrate our approach in an inventory management problem
               faced by the distribution arm of a large media company, shipping
               1 billion units yearly. We leverage both internal data and
               public data harvested from IMDb, Rotten Tomatoes, and Google to
               prescribe operational decisions that outperform baseline
               measures. Specifically, the data we collect, leveraged by our
               methods, account for an 88\% improvement as measured by our
               coefficient of prescriptiveness.This paper was accepted by Noah
               Gans, optimization.",
  journal   = "Manage. Sci.",
  publisher = "INFORMS",
  volume    =  66,
  number    =  3,
  pages     = "1025--1044",
  month     =  mar,
  year      =  2020
}

@ARTICLE{Rostami-Tabar2022-sx,
  title    = "Forecasting for social good",
  author   = "Rostami-Tabar, Bahman and Ali, Mohammad M and Hong, Tao and
              Hyndman, Rob J and Porter, Michael D and Syntetos, Aris",
  abstract = "Forecasting plays a critical role in the development of
              organisational business strategies. Despite a considerable body
              of research in the area of forecasting, the focus has largely
              been on the financial and economic outcomes of the forecasting
              process as opposed to societal benefits. Our motivation in this
              study is to promote the latter, with a view to using the
              forecasting process to advance social and environmental
              objectives such as equality, social justice and sustainability.
              We refer to such forecasting practices as Forecasting for Social
              Good (FSG) where the benefits to society and the environment take
              precedence over economic and financial outcomes. We conceptualise
              FSG and discuss its scope and boundaries in the context of the
              ``Doughnut theory''. We present some key attributes that qualify
              a forecasting process as FSG: it is concerned with a real
              problem; it is focused on advancing social and environmental
              goals and prioritises these over conventional measures of
              economic success; and it has a broad societal impact. We also
              position FSG in the wider literature on forecasting and social
              good practices. We propose an FSG maturity framework as the means
              to engage academics and practitioners with research in this area.
              Finally, we highlight that FSG: (i) cannot be distilled to a
              prescriptive set of guidelines, (ii) is scalable, and (iii) has
              the potential to make significant contributions to advancing
              social objectives.",
  journal  = "Int. J. Forecast.",
  volume   =  38,
  number   =  3,
  pages    = "1245--1257",
  month    =  jul,
  year     =  2022,
  keywords = "Forecasting; Social good; Social foundation; Ecological ceiling;
              Sustainability"
}

@ARTICLE{Kotary_undated-mq,
  title  = "{End-to-End} Constrained Optimization Learning: A Survey",
  author = "Kotary, James and Fioretto, Ferdinando and Van Hentenryck, Pascal
            and Wilder, Bryan"
}

@ARTICLE{He_undated-pg,
  title  = "Deep Reinforcement Learning for Generalizable Field Development
            Optimization",
  author = "He, Jincong and Tang, Meng and Hu, Chaoshun and Tanaka, Shusei and
            Wang, Kainan and Wen, Xian-Huan and Nasir, Yusuf and Chevron, E T C"
}

@ARTICLE{Nasir2021-ri,
  title    = "Deep Reinforcement Learning for Constrained Field Development
              Optimization in Subsurface Two-phase Flow",
  author   = "Nasir, Yusuf and He, Jincong and Hu, Chaoshun and Tanaka, Shusei
              and Wang, Kainan and Wen, Xianhuan",
  abstract = "Oil and gas field development optimization, which involves the
              determination of the optimal number of wells, their drilling
              sequence and locations while satisfying operational and economic
              constraints, represents a challenging computational problem. In
              this work, we present a deep-reinforcement-learning-based
              artificial intelligence agent that could provide optimized
              development plans given a basic description of the reservoir and
              rock/fluid properties with minimal computational cost. This
              artificial intelligence agent, comprising of a convolutional
              neural network, provides a mapping from a given state of the
              reservoir model, constraints, and economic condition to the
              optimal decision (drill/do not drill and well location) to be
              taken in the next stage of the defined sequential field
              development planning process. The state of the reservoir model is
              defined using parameters that appear in the governing equations
              of the two-phase flow (such as well index, transmissibility,
              fluid mobility, and accumulation, etc.,). A feedback loop
              training process referred to as deep reinforcement learning is
              used to train an artificial intelligence agent with such a
              capability. The training entails millions of flow simulations
              with varying reservoir model descriptions (structural, rock and
              fluid properties), operational constraints (maximum liquid
              production, drilling duration, and water-cut limit), and economic
              conditions. The parameters that define the reservoir model,
              operational constraints, and economic conditions are randomly
              sampled from a defined range of applicability. Several
              algorithmic treatments are introduced to enhance the training of
              the artificial intelligence agent. After appropriate training,
              the artificial intelligence agent provides an optimized field
              development plan instantly for new scenarios within the defined
              range of applicability. This approach has advantages over
              traditional optimization algorithms (e.g., particle swarm
              optimization, genetic algorithm) that are generally used to find
              a solution for a specific field development scenario and
              typically not generalizable to different scenarios. The
              performance of the artificial intelligence agents for two- and
              three-dimensional subsurface flow are compared to well-pattern
              agents. Optimization results using the new procedure are shown to
              significantly outperform those from the well pattern agents.",
  journal  = "Frontiers in Applied Mathematics and Statistics",
  volume   =  7,
  year     =  2021
}

@ARTICLE{Koh2021-cq,
  title    = "Utilising a deep neural network as a surrogate model to
              approximate phenomenological models of a comminution circuit for
              faster simulations",
  author   = "Koh, Edwin J Y and Amini, Eiman and McLachlan, Geoffrey J and
              Beaton, Nick",
  abstract = "Comminution circuits can be modelled using phenomenological
              models of individual unit operations to represent the operation
              performance. Process optimisation and block model variability
              analysis require millions of simulations to fully explore process
              efficiency through mine scheduling which is costly and time
              consuming. Surrogate modelling is a technique used in process
              engineering approach of approximating the behaviour of the
              underlying model by using a model which is computationally more
              feasible. Neural networks are a form of machine learning which
              can approximate complicated function mappings of inputs to
              outputs and are computationally parallelisable. In this study, a
              neural network is used as a surrogate model to approximate a
              copper porphyry mine comminution circuit for faster simulations.
              The neural network was trained to predict throughput using data
              generated from the phenomenological models of the comminution
              circuit. The optimal neural network hyperparameters were
              determined using an evolutionary algorithm to minimise
              overfitting. The neural network predicted simulation results 3363
              times quicker than phenomenological models with errors of 0.37\%,
              0.55\% and 0.45\% across three different test sets. The neural
              network only required a stratified training sample of 1 in 1000
              data to interpolate the rest of the data.",
  journal  = "Miner. Eng.",
  volume   =  170,
  pages    = "107026",
  month    =  aug,
  year     =  2021,
  keywords = "Simulation and modelling; Surrogate model; Comminution; Neural
              networks; Evolutionary algorithm"
}

@ARTICLE{Shah2022-pq,
  title         = "{Decision-Focused} Learning without {Decision-Making}:
                   Learning Locally Optimized Decision Losses",
  author        = "Shah, Sanket and Wang, Kai and Wilder, Bryan and Perrault,
                   Andrew and Tambe, Milind",
  abstract      = "Decision-Focused Learning (DFL) is a paradigm for tailoring
                   a predictive model to a downstream optimization task that
                   uses its predictions in order to perform better on that
                   specific task. The main technical challenge associated with
                   DFL is that it requires being able to differentiate through
                   the optimization problem, which is difficult due to
                   discontinuous solutions and other challenges. Past work has
                   largely gotten around this this issue by handcrafting
                   task-specific surrogates to the original optimization
                   problem that provide informative gradients when
                   differentiated through. However, the need to handcraft
                   surrogates for each new task limits the usability of DFL. In
                   addition, there are often no guarantees about the convexity
                   of the resulting surrogates and, as a result, training a
                   predictive model using them can lead to inferior local
                   optima. In this paper, we do away with surrogates altogether
                   and instead learn loss functions that capture task-specific
                   information. To the best of our knowledge, ours is the first
                   approach that entirely replaces the optimization component
                   of decision-focused learning with a loss that is
                   automatically learned. Our approach (a) only requires access
                   to a black-box oracle that can solve the optimization
                   problem and is thus generalizable, and (b) can be convex by
                   construction and so can be easily optimized over. We
                   evaluate our approach on three resource allocation problems
                   from the literature and find that our approach outperforms
                   learning without taking into account task-structure in all
                   three domains, and even hand-crafted surrogates from the
                   literature.",
  month         =  mar,
  year          =  2022,
  archivePrefix = "arXiv",
  primaryClass  = "cs.LG",
  eprint        = "2203.16067"
}

@ARTICLE{Donti_undated-sq,
  title  = "{DC3}: A {LEARNING} {METHOD} {FOR} {OPTIMIZATION} {WITH} {HARD}
            {CONSTRAINTS}",
  author = "Donti, Priya L and Rolnick, David and Zico Kolter, J"
}

@ARTICLE{Chatzos2021-ro,
  title         = "Spatial Network Decomposition for Fast and Scalable {AC-OPF}
                   Learning",
  author        = "Chatzos, Minas and Mak, Terrence W K and Van Hentenryck,
                   Pascal",
  abstract      = "This paper proposes a novel machine-learning approach for
                   predicting AC-OPF solutions that features a fast and
                   scalable training. It is motivated by the two critical
                   considerations: (1) the fact that topology optimization and
                   the stochasticity induced by renewable energy sources may
                   lead to fundamentally different AC-OPF instances; and (2)
                   the significant training time needed by existing
                   machine-learning approaches for predicting AC-OPF. The
                   proposed approach is a 2-stage methodology that exploits a
                   spatial decomposition of the power network that is viewed as
                   a set of regions. The first stage learns to predict the
                   flows and voltages on the buses and lines coupling the
                   regions, and the second stage trains, in parallel, the
                   machine-learning models for each region. Experimental
                   results on the French transmission system (up to 6,700 buses
                   and 9,000 lines) demonstrate the potential of the approach.
                   Within a short training time, the approach predicts AC-OPF
                   solutions with very high fidelity and minor constraint
                   violations, producing significant improvements over the
                   state-of-the-art. The results also show that the predictions
                   can seed a load flow optimization to return a feasible
                   solution within 0.03\% of the AC-OPF objective, while
                   reducing running times significantly.",
  month         =  jan,
  year          =  2021,
  archivePrefix = "arXiv",
  primaryClass  = "cs.LG",
  eprint        = "2101.06768"
}

@ARTICLE{Bengio2021-hs,
  title    = "Machine learning for combinatorial optimization: A methodological
              tour d'horizon",
  author   = "Bengio, Yoshua and Lodi, Andrea and Prouvost, Antoine",
  abstract = "This paper surveys the recent attempts, both from the machine
              learning and operations research communities, at leveraging
              machine learning to solve combinatorial optimization problems.
              Given the hard nature of these problems, state-of-the-art
              algorithms rely on handcrafted heuristics for making decisions
              that are otherwise too expensive to compute or mathematically not
              well defined. Thus, machine learning looks like a natural
              candidate to make such decisions in a more principled and
              optimized way. We advocate for pushing further the integration of
              machine learning and combinatorial optimization and detail a
              methodology to do so. A main point of the paper is seeing generic
              optimization problems as data points and inquiring what is the
              relevant distribution of problems to use for learning on a given
              task.",
  journal  = "Eur. J. Oper. Res.",
  volume   =  290,
  number   =  2,
  pages    = "405--421",
  month    =  apr,
  year     =  2021,
  keywords = "Combinatorial optimization; Machine learning; Branch and bound;
              Mixed-integer programming solvers"
}

@ARTICLE{Sangalli2021-iq,
  title         = "Constrained Optimization to Train Neural Networks on
                   Critical and {Under-Represented} Classes",
  author        = "Sangalli, Sara and Erdil, Ertunc and Hoetker, Andreas and
                   Donati, Olivio and Konukoglu, Ender",
  abstract      = "Deep neural networks (DNNs) are notorious for making more
                   mistakes for the classes that have substantially fewer
                   samples than the others during training. Such class
                   imbalance is ubiquitous in clinical applications and very
                   crucial to handle because the classes with fewer samples
                   most often correspond to critical cases (e.g., cancer) where
                   misclassifications can have severe consequences. Not to miss
                   such cases, binary classifiers need to be operated at high
                   True Positive Rates (TPRs) by setting a higher threshold,
                   but this comes at the cost of very high False Positive Rates
                   (FPRs) for problems with class imbalance. Existing methods
                   for learning under class imbalance most often do not take
                   this into account. We argue that prediction accuracy should
                   be improved by emphasizing reducing FPRs at high TPRs for
                   problems where misclassification of the positive, i.e.
                   critical, class samples are associated with higher cost. To
                   this end, we pose the training of a DNN for binary
                   classification as a constrained optimization problem and
                   introduce a novel constraint that can be used with existing
                   loss functions to enforce maximal area under the ROC curve
                   (AUC) through prioritizing FPR reduction at high TPR. We
                   solve the resulting constrained optimization problem using
                   an Augmented Lagrangian method (ALM). Going beyond binary,
                   we also propose two possible extensions of the proposed
                   constraint for multi-class classification problems. We
                   present experimental results for image-based binary and
                   multi-class classification applications using an in-house
                   medical imaging dataset, CIFAR10, and CIFAR100. Our results
                   demonstrate that the proposed method improves the baselines
                   in majority of the cases by attaining higher accuracy on
                   critical classes while reducing the misclassification rate
                   for the non-critical class samples.",
  month         =  feb,
  year          =  2021,
  archivePrefix = "arXiv",
  primaryClass  = "cs.LG",
  eprint        = "2102.12894"
}

@ARTICLE{Den_Hertog_undated-no,
  title  = "Bridging the gap between predictive and prescriptive analytics -
            new optimization methodology needed",
  author = "den Hertog, Dick"
}

@MISC{noauthor_undated-ij,
  title = "Bertsimas-slides 1.pdf"
}

@ARTICLE{Popescu2022-tk,
  title    = "An overview of machine learning techniques in constraint solving",
  author   = "Popescu, Andrei and Polat-Erdeniz, Seda and Felfernig, Alexander
              and Uta, Mathias and Atas, M{\"u}sl{\"u}m and Le, Viet-Man and
              Pilsl, Klaus and Enzelsberger, Martin and Tran, Thi Ngoc Trang",
  abstract = "Constraint solving is applied in different application contexts.
              Examples thereof are the configuration of complex products and
              services, the determination of production schedules, and the
              determination of recommendations in online sales scenarios.
              Constraint solvers apply, for example, search heuristics to
              assure adequate runtime performance and prediction quality.
              Several approaches have already been developed showing that
              machine learning (ML) can be used to optimize search processes in
              constraint solving. In this article, we provide an overview of
              the state of the art in applying ML approaches to constraint
              solving problems including constraint satisfaction, SAT solving,
              answer set programming (ASP) and applications thereof such as
              configuration, constraint-based recommendation, and model-based
              diagnosis. We compare and discuss the advantages and
              disadvantages of these approaches and point out relevant
              directions for future work.",
  journal  = "J. Intell. Inf. Syst.",
  volume   =  58,
  number   =  1,
  pages    = "91--118",
  month    =  feb,
  year     =  2022
}

@ARTICLE{Paul2021-yt,
  title    = "An ensemble forecasting model for predicting contribution of food
              donors based on supply behavior",
  author   = "Paul, Shubhra and Davis, Lauren B",
  abstract = "Food banks are nonprofit hunger relief organizations that collect
              donations from donors and distribute food to local agencies that
              serve people in need. Donors consist of local supermarkets,
              manufacturers, and community organizations. The frequency,
              quantity, and type of food donated by each donor can vary each
              month. In this research, we propose a technique to identify the
              supply behavior of donors and cluster them based on these
              attributes. We then develop a predictive ensemble model to
              forecast the contribution of different donor clusters. Our study
              shows the necessary behavioral attributes to classify donors and
              the best way to cluster donor data to improve the prediction
              model.",
  journal  = "Ann. Oper. Res.",
  month    =  jul,
  year     =  2021
}

@ARTICLE{Sugiyama2007-uv,
  title     = "Forecast Uncertainty and Monte Carlo Simulation",
  author    = "Sugiyama, Sam",
  abstract  = "Sam Sugiyama has written a primer on the use of Monte Carlo
               Simulation to assess forecast error. His simple illustrative
               example and description of the steps in the MCS procedure
               provide a non-technical overview of this fascinating approach to
               the evaluation of uncertainty in forecasts. For regression
               modelers specifically, Sam shows how MCS can be used to develop
               more realistic prediction intervals than the theoretical PIs
               found in books and software. Copyright International Institute
               of Forecasters, 2007",
  journal   = "Foresight: The International Journal of Applied Forecasting",
  publisher = "International Institute of Forecasters",
  number    =  6,
  pages     = "29--37",
  year      =  2007
}
